{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    div: \"div\",\n    em: \"em\",\n    h2: \"h2\",\n    hr: \"hr\",\n    li: \"li\",\n    line: \"line\",\n    p: \"p\",\n    path: \"path\",\n    pre: \"pre\",\n    svg: \"svg\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://llamaindex.ai/\",\n        children: \"LlamaIndex\"\n      }), \" est un framework open-source pour indexer des donn√©es et les rendre accessibles par un \", _jsx(_components.a, {\n        href: \"/llm\",\n        title: \"LLM\",\n        children: \"LLM\"\n      }), \".\"]\n    }), \"\\n\\n\", _jsxs(_components.p, {\n      children: [\"Cela vous permet de poser des questions √† vos fichiers textes (\", _jsx(_components.code, {\n        children: \".txt\"\n      }), \", \", _jsx(_components.code, {\n        children: \".md\"\n      }), \", \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \", \", _jsx(_components.code, {\n        children: \".epub\"\n      }), \", etc...).\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Et avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n'importe quel type de donn√©es.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il existe une \", _jsx(_components.a, {\n        href: \"https://github.com/run-llama/llama_index\",\n        children: \"version Python\"\n      }), \" et une \", _jsx(_components.a, {\n        href: \"https://github.com/run-llama/LlamaIndexTS\",\n        children: \"version JavaScript\"\n      }), \".\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      className: \"callout note\",\n      children: [\"\\n                    \\n                        \", _jsxs(_components.div, {\n        className: \"callout-title\",\n        children: [\"\\n                            \", _jsx(_components.div, {\n          className: \"callout-icon\",\n          children: _jsxs(_components.svg, {\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"24\",\n            height: \"24\",\n            viewBox: \"0 0 24 24\",\n            fill: \"none\",\n            stroke: \"currentColor\",\n            strokeWidth: \"2\",\n            strokeLinecap: \"round\",\n            strokeLinejoin: \"round\",\n            children: [_jsx(_components.line, {\n              x1: \"18\",\n              y1: \"2\",\n              x2: \"22\",\n              y2: \"6\"\n            }), _jsx(_components.path, {\n              d: \"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"\n            })]\n          })\n        }), \"\\n                            \", _jsx(_components.div, {\n          className: \"callout-title-inner\",\n          children: \"note\"\n        }), \"\\n                        \"]\n      }), \"\\n                    \\n                    \", _jsxs(_components.div, {\n        className: \"callout-content\",\n        children: [\"\\n                        \", _jsx(_components.p, {\n          children: \"Bien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d'OpenAI. Donc n'oubliez pas que chaque requ√™te √† l'API d'OpenAI est factur√©e üòâ\"\n        }), \"\\n                    \"]\n      }), \"\\n                \"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Installation\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Commencez par cr√©er un compte sur \", _jsx(_components.a, {\n        href: \"https://platform.openai.com/\",\n        children: \"OpenAI\"\n      }), \" pour pouvoir utiliser l'API GPT-3.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Puis exportez \", _jsx(_components.a, {\n        href: \"https://platform.openai.com/api-keys\",\n        children: \"votre cl√© d'API\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"export OPENAI_API_KEY=\\\"votre-cl√©-d'API\\\"\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"npm i llamaindex\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour la version Python, installez le package \", _jsx(_components.code, {\n        children: \"llama_index\"\n      }), \" avec \", _jsx(_components.code, {\n        children: \"pip\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"pip install llama_index\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation en JavaScript\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Cr√©ez un fichier \", _jsx(_components.code, {\n        children: \"index.mjs\"\n      }), \" avec le contenu suivant :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-js\",\n        children: \"// index.mjs\\nimport fs from 'fs';\\nimport { Document, VectorStoreIndex } from 'llamaindex';\\n\\nconst essay = fs.readFileSync('node_modules/llamaindex/examples/abramov.txt', 'utf-8');\\nconst document = new Document({ text: essay });\\n\\nconst index = await VectorStoreIndex.fromDocuments([document]);\\n\\nconst queryEngine = index.asQueryEngine();\\nconst response = await queryEngine.query({\\n  query: 'What did the author do in college?',\\n});\\n\\nconsole.log(response.toString());\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Cela permet de lire un fichier texte au format \", _jsx(_components.code, {\n        children: \".txt\"\n      }), \" et de poser une question sur son contenu :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"node index.mjs\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour faire la m√™me chose avec un fichier au format \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \", il faut utiliser le Data Loader \", _jsx(_components.code, {\n        children: \"PDFReader\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-js\",\n        children: \"// index.mjs\\nimport { PDFReader, VectorStoreIndex } from 'llamaindex';\\n\\nconst reader = new PDFReader();\\nconst documents = await reader.loadData('book.pdf');\\n\\nconst index = await VectorStoreIndex.fromDocuments(documents);\\n\\nconst queryEngine = index.asQueryEngine();\\nconst response = await queryEngine.query({\\n  query: 'Give me a summary of this book',\\n});\\n\\nconsole.log(response.toString());\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation en Python\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour Python, cr√©ez un dossier \", _jsx(_components.code, {\n        children: \"data\"\n      }), \" avec des fichiers texte au format \", _jsx(_components.code, {\n        children: \".txt\"\n      }), \" ou \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \" et le fichier \", _jsx(_components.code, {\n        children: \"index.py\"\n      }), \" suivant :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"# index.py\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of this book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Voici un exemple pour charger un autre type de fichier (ex: \", _jsx(_components.code, {\n        children: \".epub\"\n      }), \") :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex\\nfrom llama_index.readers.file import EpubReader\\n\\ndocuments = EpubReader().load_data(\\\"./book.epub\\\")\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour persister l'index, et √©viter d'indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (\", _jsx(_components.code, {\n        children: \"index.py\"\n      }), \" et \", _jsx(_components.code, {\n        children: \"query.py\"\n      }), \") et utiliser la m√©thode \", _jsx(_components.code, {\n        children: \"save\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"# index.py\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\nindex.storage_context.persist()\\n\\n# query.py\\nfrom llama_index.core import StorageContext, load_index_from_storage\\n\\nstorage_context = StorageContext.from_defaults(persist_dir=\\\"./storage\\\")\\nindex = load_index_from_storage(storage_context)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour utiliser un autre \", _jsx(_components.a, {\n        href: \"/llm\",\n        title: \"LLM\",\n        children: \"LLM\"\n      }), \" que GPT-3, vous pouvez utiliser \", _jsx(_components.a, {\n        href: \"/ollama\",\n        title: \"Ollama\",\n        children: \"Ollama\"\n      }), \" avec l'argument \", _jsx(_components.code, {\n        children: \"llm\"\n      }), \" de la fonction \", _jsx(_components.code, {\n        children: \"as_query_engine\"\n      }), \" :\"]\n    }), \"\\n\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex\\nfrom llama_index.readers.file import EpubReader\\nfrom llama_index.llms.ollama import Ollama\\n\\ndocuments = EpubReader().load_data(\\\"./book.epub\\\")\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nllama = Ollama(\\n    model=\\\"llama2\\\",\\n    request_timeout=40.0,\\n)\\n\\nquery_engine = index.as_query_engine(llm=llama)\\nres = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(res)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Et pour utiliser un autre \", _jsx(_components.em, {\n        children: \"embedding model\"\n      }), \" que celui d'OpenAI :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\\\"mixedbread-ai/mxbai-embed-large-v1\\\")\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\\nindex.storage_context.persist()\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"L'avantage avec Python, c'est qu'il existe beaucoup plus de \", _jsx(_components.a, {\n        href: \"https://llamahub.ai/?tab=readers\",\n        children: \"Data Loaders\"\n      }), \" qu'avec JavaScript pour charger diff√©rents types de donn√©es :\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"EPUBReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".epub\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"VideoAudioReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".mp4\"\n        }), \" et \", _jsx(_components.code, {\n          children: \".mp3\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"ImageReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".png\"\n        }), \" et \", _jsx(_components.code, {\n          children: \".jpg\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"RSSReader\"\n        }), \" (pour les flux RSS)\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"etc...\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour aller plus loin, voici \", _jsx(_components.a, {\n        href: \"https://docs.llamaindex.ai/en/stable/\",\n        children: \"la documentation de LlamaIndex\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"R√©f√©rences :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/run-llama/llama_index\",\n          children: \"https://github.com/run-llama/llama_index\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://huggingface.co/learn/cookbook/rag_llamaindex_librarian\",\n          children: \"https://huggingface.co/learn/cookbook/rag_llamaindex_librarian\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb\",\n          children: \"https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{"title":"LlamaIndex","permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true},"scope":{}},"isIndex":false,"file":"LlamaIndex.md","fileName":"LlamaIndex","comments":true,"title":"LlamaIndex","permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true},"__N_SSG":true}