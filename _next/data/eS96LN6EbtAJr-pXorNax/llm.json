{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    hr: \"hr\",\n    li: \"li\",\n    p: \"p\",\n    strong: \"strong\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Un LLM (Large Language Model) est un modèle de langage de grande taille.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Il est capable de générer du texte et d'effectuer des tâches de traitement du langage naturel telles que :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"La génération de texte\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"La traduction de langues\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"La classification de texte\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"La réponse à des questions\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Etc...\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Le plus connu des LLM propriètaires est GPT-4, développé par OpenAI.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Il existe aussi des LLM open-source comme Mistral de Mistral.ai ou LLama3 de Meta.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Chaque modèle a une taille différente (7B, 13B, 34B, 70B, 110B, 400B), qui correspond au nombre de milliards de paramètres qu'il possède.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour vulgariser, le \", _jsx(_components.strong, {\n        children: \"nombre de paramètres\"\n      }), \" est l'équivalent du QI d'un modèle.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Plus le modèle est grand et plus cela demande de ressources GPU/RAM pour l'utiliser.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Aussi, chaque LLM a une \", _jsx(_components.strong, {\n        children: \"fenêtre de contexte\"\n      }), \" (\", _jsx(_components.a, {\n        href: \"/context-window\",\n        title: \"Context window\",\n        className: \"not-found\",\n        children: \"Context window\"\n      }), \") qui correspond au nombre de tokens que peut prendre le modèle en entrée.\"]\n    }), \"\\n\\n\", _jsx(_components.p, {\n      children: \"Par exemple :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"LLama3 -> 8,000 tokens\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Mixtral 8x22B's -> 64,000 tokens\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"GPT-4 Turbo -> 128,000 tokens\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Command-R+ -> 128,000 tokens\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Claude 3 -> 200,000 tokens\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Gemini 1.5 Pro -> 2,000,000 tokens\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Grâce à une fenêtre de contexte plus grande, le modèle peut par exemple prendre un livre entier en entrée et générer un texte qui a du sens.\"\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"Références :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/rasbt/LLMs-from-scratch\",\n          children: \"https://github.com/rasbt/LLMs-from-scratch\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math\",\n          children: \"https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{"aliases":["Large Language Model"],"title":"LLM","permalink":"llm","description":"Un LLM (Large Language Model) est un modèle de langage de grande taille.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-05-15T10:00:00","publish":true},"scope":{}},"isIndex":false,"file":"LLM.md","fileName":"LLM","comments":true,"title":"LLM","permalink":"llm","aliases":["Large Language Model"],"description":"Un LLM (Large Language Model) est un modèle de langage de grande taille.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-05-15T10:00:00","publish":true},"__N_SSG":true}