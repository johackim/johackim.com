{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    div: \"div\",\n    h2: \"h2\",\n    hr: \"hr\",\n    img: \"img\",\n    li: \"li\",\n    line: \"line\",\n    p: \"p\",\n    path: \"path\",\n    pre: \"pre\",\n    svg: \"svg\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://github.com/ollama/ollama\",\n        children: \"Ollama\"\n      }), \" est un outil qui permet d'utiliser des modèles d'IA (Llama 2, Mistral, Gemma, etc...) localement sur son propre ordinateur ou serveur.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"C'est ultra simple à utiliser, et ça permet de tester des modèles d'IA sans être un expert en IA.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il supporte un grand nombre de \", _jsx(_components.a, {\n        href: \"https://ollama.ai/library\",\n        children: \"modèles d'IA\"\n      }), \" donc certains en version non censurés.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rien de mieux pour tester des modèles d'IA non propriétaires !\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Installation\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour l'installer sur Linux :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"curl -fsSL https://ollama.com/install.sh | sh\\n# Ou\\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.1.32 sh # Pour une version spécifique\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour l'installer sur Arch Linux :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo pacman -S ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour démarrer le service ollama :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo systemctl start ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour démarrer un modèle d'IA, il suffit de lancer la commande \", _jsx(_components.code, {\n        children: \"ollama run\"\n      }), \" suivi du nom du modèle.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Par exemple, pour démarrer \", _jsx(_components.a, {\n        href: \"https://mistral.ai\",\n        children: \"Mistral\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama run mistral\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Une fois le modèle démarré, vous pouvez directement interagir avec lui depuis votre terminal.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour supprimer le modèle :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama rm mistral\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il existe même une commande pour démarrer Ollama en mode serveur avec \", _jsx(_components.a, {\n        href: \"https://hub.docker.com/r/ollama/ollama\",\n        children: \"Docker\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"docker run -d --name ollama --restart=always -v ~/.ollama:/root/.ollama -p 11434:11434 ollama/ollama\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vous pouvez interagir avec Ollama via le port \", _jsx(_components.code, {\n        children: \"11434\"\n      }), \" avec des requêtes HTTP :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"curl -X POST http://localhost:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral\\\",\\n  \\\"prompt\\\":\\\"Here is a story about llamas eating grass\\\"\\n}'\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation des modèles HuggingFace au format .gguf\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Et si vous voulez utiliser \", _jsx(_components.a, {\n        href: \"https://huggingface.co/models?search=gguf\",\n        children: \"un modèle au format .gguf\"\n      }), \", vous pouvez le faire :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation avec un client web\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il est aussi possible d'utiliser un client web comme \", _jsx(_components.a, {\n        href: \"https://github.com/open-webui/open-webui\",\n        children: \"Open WebUI\"\n      }), \", \", _jsx(_components.a, {\n        href: \"https://github.com/mckaywrigley/chatbot-ui\",\n        children: \"Chatbot UI\"\n      }), \" ou \", _jsx(_components.a, {\n        href: \"https://github.com/lobehub/lobe-chat\",\n        children: \"Lobe Chat\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"https://i.imgur.com/dprcPpW.png\",\n        alt: \"Open WebUI\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cela donne un rendu très équivalent à ChatGPT.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Nos données restent privées et l'on peut discuter avec un modèle d'IA sans être censuré.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Désinstallation\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour désintaller Ollama :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo systemctl disable --now ollama\\nsudo rm -rf /var/lib/ollama\\nsudo pacman -Rsn ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Serverless GPU\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      className: \"callout note\",\n      children: [\"\\n                    \\n                        \", _jsxs(_components.div, {\n        className: \"callout-title\",\n        children: [\"\\n                            \", _jsx(_components.div, {\n          className: \"callout-icon\",\n          children: _jsxs(_components.svg, {\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"24\",\n            height: \"24\",\n            viewBox: \"0 0 24 24\",\n            fill: \"none\",\n            stroke: \"currentColor\",\n            strokeWidth: \"2\",\n            strokeLinecap: \"round\",\n            strokeLinejoin: \"round\",\n            children: [_jsx(_components.line, {\n              x1: \"18\",\n              y1: \"2\",\n              x2: \"22\",\n              y2: \"6\"\n            }), _jsx(_components.path, {\n              d: \"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"\n            })]\n          })\n        }), \"\\n                            \", _jsx(_components.div, {\n          className: \"callout-title-inner\",\n          children: \"note\"\n        }), \"\\n                        \"]\n      }), \"\\n                    \\n                    \", _jsxs(_components.div, {\n        className: \"callout-content\",\n        children: [\"\\n                        \", _jsx(_components.p, {\n          children: \"En cours de création\"\n        }), \"\\n                    \"]\n      }), \"\\n                \"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Comment utiliser \", _jsx(_components.code, {\n        children: \"ollama run <model>\"\n      }), \" (ou open-webui) avec un serveur GPU distant uniquement lorsque une requête est envoyé ?\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"OLLAMA_HOST=https://my.proxy.com ollama run deepseek-r1\\n# Use a proxy\\n# Use runpod\\n# \\\"Ollama is running\\\" on http://<runpod_ip>:11434/\\n# https://github.com/marknefedov/ollama-openrouter-proxy\\n\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"Références :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.geeek.org/mistral-ollama/\",\n          children: \"https://www.geeek.org/mistral-ollama/\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/\",\n          children: \"https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama\",\n          children: \"https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{"title":"Ollama","permalink":"ollama","description":"Ollama est un outil pour utiliser des modèles d'IA (Llama 2, Mistral, etc...) localement.","datePublished":"2024-02-19T10:00:00","dateUpdated":"2024-10-16T10:00:00","publish":true,"rss":true},"scope":{}},"isIndex":false,"file":"Ollama.md","fileName":"Ollama","title":"Ollama","comments":true,"permalink":"ollama","description":"Ollama est un outil pour utiliser des modèles d'IA (Llama 2, Mistral, etc...) localement.","datePublished":"2024-02-19T10:00:00","dateUpdated":"2024-10-16T10:00:00","publish":true,"rss":true},"__N_SSG":true}