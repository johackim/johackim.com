{"pageProps":{"source":"<div class=\"space-y-4 whitespace-normal *:first:mt-0 *:last:mb-0\"><p><a class=\"\" target=\"_blank\" href=\"https://llamaindex.ai/\">LlamaIndex</a> est un framework open-source pour indexer des donn√©es et les rendre accessibles par un <a class=\"\" href=\"/llm\">LLM</a>.</p>\n<p>Cela vous permet de poser des questions √† vos fichiers textes (<code>.txt</code>, <code>.md</code>, <code>.pdf</code>, <code>.epub</code>, etc...).</p><p>Et avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n&#x27;importe quel type de donn√©es.</p><p>Il existe une <a class=\"\" target=\"_blank\" href=\"https://github.com/run-llama/llama_index\">version Python</a> et une <a class=\"\" target=\"_blank\" href=\"https://github.com/run-llama/LlamaIndexTS\">version JavaScript</a>.</p><blockquote class=\"callout note\">\n                    \n                        <div class=\"callout-title\">\n                            <div class=\"callout-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><line x1=\"18\" y1=\"2\" x2=\"22\" y2=\"6\"></line><path d=\"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"></path></svg></div>\n                            <div class=\"callout-title-inner\">note</div>\n                        </div>\n                    \n                    <div class=\"callout-content\">\n                        <p>Bien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d&#x27;OpenAI. Donc n&#x27;oubliez pas que chaque requ√™te √† l&#x27;API d&#x27;OpenAI est factur√©e üòâ</p>\n                    </div>\n                </blockquote><h2>Installation</h2><p>Commencez par cr√©er un compte sur <a class=\"\" target=\"_blank\" href=\"https://platform.openai.com/\">OpenAI</a> pour pouvoir utiliser l&#x27;API GPT-3.</p><p>Puis exportez <a class=\"\" target=\"_blank\" href=\"https://platform.openai.com/api-keys\">votre cl√© d&#x27;API</a> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">export OPENAI_API_KEY=&quot;votre-cl√©-d&#x27;API&quot;\n</code></pre></div><p>Pour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">npm i llamaindex\n</code></pre></div><p>Pour la version Python, installez le package <code>llama_index</code> avec <code>pip</code> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">pip install llama_index\n</code></pre></div><h2>Utilisation en JavaScript</h2><p>Cr√©ez un fichier <code>index.mjs</code> avec le contenu suivant :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-js\">// index.mjs\nimport fs from &#x27;fs&#x27;;\nimport { Document, VectorStoreIndex } from &#x27;llamaindex&#x27;;\n\nconst essay = fs.readFileSync(&#x27;node_modules/llamaindex/examples/abramov.txt&#x27;, &#x27;utf-8&#x27;);\nconst document = new Document({ text: essay });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: &#x27;What did the author do in college?&#x27;,\n});\n\nconsole.log(response.toString());\n</code></pre></div><p>Cela permet de lire un fichier texte au format <code>.txt</code> et de poser une question sur son contenu :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">node index.mjs\n</code></pre></div><p>Pour faire la m√™me chose avec un fichier au format <code>.pdf</code>, il faut utiliser le Data Loader <code>PDFReader</code> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-js\">// index.mjs\nimport { PDFReader, VectorStoreIndex } from &#x27;llamaindex&#x27;;\n\nconst reader = new PDFReader();\nconst documents = await reader.loadData(&#x27;book.pdf&#x27;);\n\nconst index = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: &#x27;Give me a summary of this book&#x27;,\n});\n\nconsole.log(response.toString());\n</code></pre></div><h2>Utilisation en Python</h2><p>Pour Python, cr√©ez un dossier <code>data</code> avec des fichiers texte au format <code>.txt</code> ou <code>.pdf</code> et le fichier <code>index.py</code> suivant :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-python\"># index.py\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(&quot;./data&quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(&quot;Give me a summary of this book&quot;)\n\nprint(response)\n</code></pre></div><p>Voici un exemple pour charger un autre type de fichier (ex: <code>.epub</code>) :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-python\">from llama_index.core import VectorStoreIndex\nfrom llama_index.readers.file import EpubReader\n\ndocuments = EpubReader().load_data(&quot;./book.epub&quot;)\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(&quot;Give me a summary of the book&quot;)\n\nprint(response)\n</code></pre></div><p>Pour persister l&#x27;index, et √©viter d&#x27;indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (<code>index.py</code> et <code>query.py</code>) et utiliser la m√©thode <code>save</code> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-python\"># index.py\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(&quot;./data&quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex.storage_context.persist()\n\n# query.py\nfrom llama_index.core import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=&quot;./storage&quot;)\nindex = load_index_from_storage(storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(&quot;Give me a summary of the book&quot;)\n\nprint(response)\n</code></pre></div><p>Pour utiliser un autre <a class=\"\" href=\"/llm\">LLM</a> que GPT-3, vous pouvez utiliser <a class=\"\" href=\"/ollama\">Ollama</a> avec l&#x27;argument <code>llm</code> de la fonction <code>as_query_engine</code> :</p>\n<div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-python\">from llama_index.core import VectorStoreIndex\nfrom llama_index.readers.file import EpubReader\nfrom llama_index.llms.ollama import Ollama\n\ndocuments = EpubReader().load_data(&quot;./book.epub&quot;)\nindex = VectorStoreIndex.from_documents(documents)\n\nllama = Ollama(\n    model=&quot;llama2&quot;,\n    request_timeout=40.0,\n)\n\nquery_engine = index.as_query_engine(llm=llama)\nres = query_engine.query(&quot;Give me a summary of the book&quot;)\n\nprint(res)\n</code></pre></div><p>Et pour utiliser un autre <em>embedding model</em> que celui d&#x27;OpenAI :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-python\">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nembed_model = HuggingFaceEmbedding(model_name=&quot;mixedbread-ai/mxbai-embed-large-v1&quot;)\n\ndocuments = SimpleDirectoryReader(&quot;./data&quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\nindex.storage_context.persist()\n</code></pre></div><p>L&#x27;avantage avec Python, c&#x27;est qu&#x27;il existe beaucoup plus de <a class=\"\" target=\"_blank\" href=\"https://llamahub.ai/?tab=readers\">Data Loaders</a> qu&#x27;avec JavaScript pour charger diff√©rents types de donn√©es :</p><ul>\n<li><code>EPUBReader</code> (pour les fichiers <code>.epub</code>)</li>\n<li><code>VideoAudioReader</code> (pour les fichiers <code>.mp4</code> et <code>.mp3</code>)</li>\n<li><code>ImageReader</code> (pour les fichiers <code>.png</code> et <code>.jpg</code>)</li>\n<li><code>RSSReader</code> (pour les flux RSS)</li>\n<li>etc...</li>\n</ul><p>Pour aller plus loin, voici <a class=\"\" target=\"_blank\" href=\"https://docs.llamaindex.ai/en/stable/\">la documentation de LlamaIndex</a>.</p><hr/><p>R√©f√©rences :</p><ul>\n<li>https://github.com/run-llama/llama_index</li>\n<li>https://huggingface.co/learn/cookbook/rag_llamaindex_librarian</li>\n<li>https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb</li>\n</ul></div>","isIndex":false,"file":"LlamaIndex.md","fileName":"LlamaIndex","title":"LlamaIndex","comments":true,"permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true,"note":"91"},"__N_SSG":true}