{"pageProps":{"source":"<link rel=\"preload\" as=\"image\" href=\"https://i.imgur.com/dprcPpW.png\"/><div class=\"space-y-4 whitespace-normal *:first:mt-0 *:last:mb-0\"><p><a class=\"\" target=\"_blank\" href=\"https://github.com/ollama/ollama\">Ollama</a> est un outil qui permet d&#x27;utiliser des modèles d&#x27;IA (Llama 2, Mistral, Gemma, etc...) localement sur son propre ordinateur ou serveur.</p><p>C&#x27;est ultra simple à utiliser, et ça permet de tester des modèles d&#x27;IA sans être un expert en IA.</p><p>Il supporte un grand nombre de <a class=\"\" target=\"_blank\" href=\"https://ollama.ai/library\">modèles d&#x27;IA</a> donc certains en version non censurés.</p><p>Rien de mieux pour tester des modèles d&#x27;IA non propriétaires !</p><h2>Installation</h2><p>Pour l&#x27;installer sur Linux :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">curl -fsSL https://ollama.com/install.sh | sh\n# Ou\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.1.32 sh # Pour une version spécifique\n</code></pre></div><p>Pour l&#x27;installer sur Arch Linux :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">sudo pacman -S ollama\n</code></pre></div><p>Pour démarrer le service ollama :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">sudo systemctl start ollama\n</code></pre></div><h2>Utilisation</h2><p>Pour démarrer un modèle d&#x27;IA, il suffit de lancer la commande <code>ollama run</code> suivi du nom du modèle.</p><p>Par exemple, pour démarrer <a class=\"\" target=\"_blank\" href=\"https://mistral.ai\">Mistral</a> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">ollama run mistral\n</code></pre></div><p>Une fois le modèle démarré, vous pouvez directement interagir avec lui depuis votre terminal.</p><p>Pour supprimer le modèle :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">ollama rm mistral\n</code></pre></div><p>Il existe même une commande pour démarrer Ollama en mode serveur avec <a class=\"\" target=\"_blank\" href=\"https://hub.docker.com/r/ollama/ollama\">Docker</a> :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">docker run -d --name ollama --restart=always -v ~/.ollama:/root/.ollama -p 11434:11434 ollama/ollama\n</code></pre></div><p>Vous pouvez interagir avec Ollama via le port <code>11434</code> avec des requêtes HTTP :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">curl -X POST http://localhost:11434/api/generate -d &#x27;{\n  &quot;model&quot;: &quot;mistral&quot;,\n  &quot;prompt&quot;:&quot;Here is a story about llamas eating grass&quot;\n}&#x27;\n</code></pre></div><h2>Utilisation des modèles HuggingFace au format .gguf</h2><p>Et si vous voulez utiliser <a class=\"\" target=\"_blank\" href=\"https://huggingface.co/models?search=gguf\">un modèle au format .gguf</a>, vous pouvez le faire :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n</code></pre></div><h2>Utilisation avec un client web</h2><p>Il est aussi possible d&#x27;utiliser un client web comme <a class=\"\" target=\"_blank\" href=\"https://github.com/open-webui/open-webui\">Open WebUI</a>, <a class=\"\" target=\"_blank\" href=\"https://github.com/mckaywrigley/chatbot-ui\">Chatbot UI</a> ou <a class=\"\" target=\"_blank\" href=\"https://github.com/lobehub/lobe-chat\">Lobe Chat</a></p><p><img src=\"https://i.imgur.com/dprcPpW.png\" alt=\"Open WebUI\"/></p><p>Cela donne un rendu très équivalent à ChatGPT.</p><p>Nos données restent privées et l&#x27;on peut discuter avec un modèle d&#x27;IA sans être censuré.</p><h2>Désinstallation</h2><p>Pour désintaller Ollama :</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">sudo systemctl disable --now ollama\nsudo rm -rf /var/lib/ollama\nsudo pacman -Rsn ollama\n</code></pre></div><h2>Serverless GPU</h2><blockquote class=\"callout note\">\n                    \n                        <div class=\"callout-title\">\n                            <div class=\"callout-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><line x1=\"18\" y1=\"2\" x2=\"22\" y2=\"6\"></line><path d=\"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"></path></svg></div>\n                            <div class=\"callout-title-inner\">note</div>\n                        </div>\n                    \n                    <div class=\"callout-content\">\n                        <p>En cours de création</p>\n                    </div>\n                </blockquote><p>Comment utiliser <code>ollama run &lt;model&gt;</code> (ou open-webui) avec un serveur GPU distant uniquement lorsque une requête est envoyé ?</p><div class=\"relative group\"><button type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\">Copier</button><pre><code class=\"language-bash\">OLLAMA_HOST=https://my.proxy.com ollama run deepseek-r1\n# Use a proxy\n# Use runpod\n# &quot;Ollama is running&quot; on http://&lt;runpod_ip&gt;:11434/\n# https://github.com/marknefedov/ollama-openrouter-proxy\n</code></pre></div><hr/><p>Références :</p><ul>\n<li>https://www.geeek.org/mistral-ollama/</li>\n<li>https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/</li>\n<li>https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama</li>\n</ul></div>","isIndex":false,"file":"Ollama.md","fileName":"Ollama","title":"Ollama","comments":true,"permalink":"ollama","description":"Ollama est un outil pour utiliser des modèles d'IA (Llama 2, Mistral, etc...) localement.","datePublished":"2024-02-19T10:00:00","dateUpdated":"2024-10-16T10:00:00","publish":true,"rss":true},"__N_SSG":true}