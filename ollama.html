<!DOCTYPE html><html lang="fr"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@_johackim" data-next-head=""/><meta name="twitter:creator" content="@_johackim" data-next-head=""/><meta property="og:url" content="https://johackim.com" data-next-head=""/><meta property="og:locale" content="fr_FR" data-next-head=""/><meta property="og:site_name" content="johackim" data-next-head=""/><title data-next-head="">Ollama | Johackim - Hacker indépendant</title><meta name="robots" content="index,follow" data-next-head=""/><meta name="description" content="Ollama est un outil pour utiliser des modèles d&#x27;IA (Llama 2, Mistral, etc...) localement." data-next-head=""/><meta property="og:title" content="Ollama" data-next-head=""/><meta property="og:description" content="Ollama est un outil pour utiliser des modèles d&#x27;IA (Llama 2, Mistral, etc...) localement." data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="article:published_time" content="2024-02-19T10:00:00.000Z" data-next-head=""/><meta property="article:modified_time" content="2024-10-16T10:00:00.000Z" data-next-head=""/><meta property="og:image" content="https://johackim.com/covers/ollama.jpg" data-next-head=""/><link rel="preload" href="/_next/static/media/47cbc4e2adbc5db9-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/a1ff13823887c96f.css" as="style"/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"article","datePublished":"2024-02-19T10:00:00.000Z","description":"Ollama est un outil pour utiliser des modèles d&apos;IA (Llama 2, Mistral, etc...) localement.","mainEntityOfPage":{"@type":"WebPage","@id":"https://johackim.com/ollama"},"headline":"Ollama","image":["https://johackim.com/covers/ollama.jpg"],"dateModified":"2024-10-16T10:00:00.000Z","author":{"@type":"Person","name":"Johackim"}}</script><link rel="stylesheet" href="/_next/static/css/a1ff13823887c96f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8cac0b4b405cede1.js" defer=""></script><script src="/_next/static/chunks/framework-b1e5f14688f9ffe6.js" defer=""></script><script src="/_next/static/chunks/main-d50aa070ebba4b51.js" defer=""></script><script src="/_next/static/chunks/pages/_app-5505c828b8556d55.js" defer=""></script><script src="/_next/static/chunks/92-8aaa74fb1c3ae5c6.js" defer=""></script><script src="/_next/static/chunks/pages/%5B%5B...permalink%5D%5D-2a343ef7f89ee26a.js" defer=""></script><script src="/_next/static/waumMbaWDFC6597IkvKuH/_buildManifest.js" defer=""></script><script src="/_next/static/waumMbaWDFC6597IkvKuH/_ssgManifest.js" defer=""></script><style id="__jsx-2490803925">html{font-family:'Roboto', 'Roboto Fallback'}</style></head><body><link rel="preload" as="image" href="https://i.imgur.com/dprcPpW.png"/><div id="__next"><header class="flex shadow-md inset-x-0 h-16 items-center z-30 text-gray-700 bg-white fixed top-0"><div class="container m-auto flex items-center justify-between flex-wrap px-4 lg:max-w-screen-lg"><a href="/"><div class="flex items-center"><img alt="logo" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="rounded-full w-12" style="color:transparent" src="/profile.jpg"/><div class="ml-2"><div class="font-bold leading-none">Johackim</div><div class="text-sm leading-none">Hacker indépendant</div></div></div></a><nav class="grid grid-flow-col gap-4 items-center"><a class="hover:underline hidden md:block" href="/">Accueil</a><a class="hover:underline hidden md:block" href="/articles">Articles</a><a href="/newsletter"><button type="button" class="bg-cyan-700 text-white hover:text-white hover:bg-cyan-800 px-2.5 py-1.5 rounded-md cursor-pointer">S&#x27;abonner</button></a></nav></div></header><main class="lg:max-w-screen-lg m-auto px-4"><div class="bg-cyan-600 h-1 z-30 fixed inset-0" style="width:0%"></div><div class="md:border md:border-gray-200 mt-20"><h1 class="h-64 flex flex-col justify-center relative border-b border-gray-200"><span class="absolute inset-0 bg-gray-100 opacity-80 z-10"></span><span class="transform text-center font-bold px-4 text-4xl text-gray-600 break-words z-20">Ollama</span></h1><div class="text-xs my-4 md:px-4 text-gray-700"><span>Mis à jour le </span><span>mercredi 16 octobre 2024</span><span> par <a class="underline text-cyan-700" href="/a-propos">johackim</a></span></div><article class="prose break-words prose-a:font-normal prose-a:text-cyan-700 prose-a:break-words marker:text-gray-700 prose-code:font-normal prose-code:break-words prose-inline-code:px-1.5 prose-inline-code:py-0.5 prose-code:whitespace-pre-wrap prose-code:text-xs prose-code:bg-gray-200 prose-code:rounded-md prose-pre:bg-gray-200 prose-pre:text-gray-700 prose-pre:overflow-x-auto max-w-none px-0 py-4 md:p-4 prose-code:before:hidden prose-code:after:hidden prose-mark:bg-gray-300 prose-td:border-gray-300 prose-td:border prose-td:px-4 prose-th:border prose-th:border-gray-300 prose-th:px-4 prose-th:py-2"><p><a class="" target="_blank" href="https://github.com/ollama/ollama">Ollama</a> est un outil qui permet d&#x27;utiliser des modèles d&#x27;IA (Llama 2, Mistral, Gemma, etc...) localement sur son propre ordinateur ou serveur.</p>
<p>C&#x27;est ultra simple à utiliser, et ça permet de tester des modèles d&#x27;IA sans être un expert en IA.</p>
<p>Il supporte un grand nombre de <a class="" target="_blank" href="https://ollama.ai/library">modèles d&#x27;IA</a> donc certains en version non censurés.</p>
<p>Rien de mieux pour tester des modèles d&#x27;IA non propriétaires !</p>
<h2>Installation</h2>
<p>Pour l&#x27;installer sur Linux :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
# Ou
curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.1.32 sh # Pour une version spécifique
</code></pre></div>
<p>Pour l&#x27;installer sur Arch Linux :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">sudo pacman -S ollama
</code></pre></div>
<p>Pour démarrer le service ollama :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">sudo systemctl start ollama
</code></pre></div>
<h2>Utilisation</h2>
<p>Pour démarrer un modèle d&#x27;IA, il suffit de lancer la commande <code>ollama run</code> suivi du nom du modèle.</p>
<p>Par exemple, pour démarrer <a class="" target="_blank" href="https://mistral.ai">Mistral</a> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">ollama run mistral
</code></pre></div>
<p>Une fois le modèle démarré, vous pouvez directement interagir avec lui depuis votre terminal.</p>
<p>Pour supprimer le modèle :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">ollama rm mistral
</code></pre></div>
<p>Il existe même une commande pour démarrer Ollama en mode serveur avec <a class="" target="_blank" href="https://hub.docker.com/r/ollama/ollama">Docker</a> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">docker run -d --name ollama --restart=always -v ~/.ollama:/root/.ollama -p 11434:11434 ollama/ollama
</code></pre></div>
<p>Vous pouvez interagir avec Ollama via le port <code>11434</code> avec des requêtes HTTP :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">curl -X POST http://localhost:11434/api/generate -d &#x27;{
  &quot;model&quot;: &quot;mistral&quot;,
  &quot;prompt&quot;:&quot;Here is a story about llamas eating grass&quot;
}&#x27;
</code></pre></div>
<h2>Utilisation des modèles HuggingFace au format .gguf</h2>
<p>Et si vous voulez utiliser <a class="" target="_blank" href="https://huggingface.co/models?search=gguf">un modèle au format .gguf</a>, vous pouvez le faire :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
</code></pre></div>
<h2>Utilisation avec un client web</h2>
<p>Il est aussi possible d&#x27;utiliser un client web comme <a class="" target="_blank" href="https://github.com/open-webui/open-webui">Open WebUI</a>, <a class="" target="_blank" href="https://github.com/mckaywrigley/chatbot-ui">Chatbot UI</a> ou <a class="" target="_blank" href="https://github.com/lobehub/lobe-chat">Lobe Chat</a></p>
<p><img src="https://i.imgur.com/dprcPpW.png" alt="Open WebUI"/></p>
<p>Cela donne un rendu très équivalent à ChatGPT.</p>
<p>Nos données restent privées et l&#x27;on peut discuter avec un modèle d&#x27;IA sans être censuré.</p>
<h2>Désinstallation</h2>
<p>Pour désintaller Ollama :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">sudo systemctl disable --now ollama
sudo rm -rf /var/lib/ollama
sudo pacman -Rsn ollama
</code></pre></div>
<h2>Serverless GPU</h2>
<blockquote class="callout note">
                    
                        <div class="callout-title">
                            <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="2" x2="22" y2="6"></line><path d="M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z"></path></svg></div>
                            <div class="callout-title-inner">note</div>
                        </div>
                    
                    <div class="callout-content">
                        <p>En cours de création</p>
                    </div>
                </blockquote>
<p>Comment utiliser <code>ollama run &lt;model&gt;</code> (ou open-webui) avec un serveur GPU distant uniquement lorsque une requête est envoyé ?</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">OLLAMA_HOST=https://my.proxy.com ollama run deepseek-r1
# Use a proxy
# Use runpod
# &quot;Ollama is running&quot; on http://&lt;runpod_ip&gt;:11434/
# https://github.com/marknefedov/ollama-openrouter-proxy
</code></pre></div>
<hr/>
<p>Références :</p>
<ul>
<li><a class="" target="_blank" href="https://www.geeek.org/mistral-ollama/">https://www.geeek.org/mistral-ollama/</a></li>
<li><a class="" target="_blank" href="https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/">https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/</a></li>
<li><a class="" target="_blank" href="https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama">https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama</a></li>
</ul></article><div class="md:px-4"><hr class="border-gray-200 mb-4 mt-8"/><div id="commento"></div></div></div></main><footer class="container m-auto px-4 py-8 flex items-center justify-between flex-wrap lg:max-w-screen-lg text-gray-700"><div><span class="inline-block transform rotate-180">©</span><span class="ml-2">2017-2025</span></div><nav class="grid grid-flow-col gap-2 items-center"><a href="https://x.com/_johackim" aria-label="X"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a><a href="https://t.me/johackim" aria-label="Telegram"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z"></path></svg></a><a href="https://mastodon.ethibox.fr/@johackim" aria-label="Mastodon"><svg class="h-5 w-5 fill-current hover:text-black" aria-hidden="true" data-prefix="fab" data-icon="mastodon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"></path></svg></a><a href="https://github.com/johackim" aria-label="Github"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a href="https://linkedin.com/in/johackim" aria-label="LinkedIn"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a href="mailto:contact+blog@johackim.com" aria-label="Mail"><svg class="h-5 w-5 fill-current hover:text-black" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="/rss.xml" aria-label="Rss"><svg class="h-5 w-5 fill-current hover:text-black" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg></a></nav></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    div: \"div\",\n    h2: \"h2\",\n    hr: \"hr\",\n    img: \"img\",\n    li: \"li\",\n    line: \"line\",\n    p: \"p\",\n    path: \"path\",\n    pre: \"pre\",\n    svg: \"svg\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://github.com/ollama/ollama\",\n        children: \"Ollama\"\n      }), \" est un outil qui permet d'utiliser des modèles d'IA (Llama 2, Mistral, Gemma, etc...) localement sur son propre ordinateur ou serveur.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"C'est ultra simple à utiliser, et ça permet de tester des modèles d'IA sans être un expert en IA.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il supporte un grand nombre de \", _jsx(_components.a, {\n        href: \"https://ollama.ai/library\",\n        children: \"modèles d'IA\"\n      }), \" donc certains en version non censurés.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rien de mieux pour tester des modèles d'IA non propriétaires !\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Installation\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour l'installer sur Linux :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"curl -fsSL https://ollama.com/install.sh | sh\\n# Ou\\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.1.32 sh # Pour une version spécifique\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour l'installer sur Arch Linux :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo pacman -S ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour démarrer le service ollama :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo systemctl start ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour démarrer un modèle d'IA, il suffit de lancer la commande \", _jsx(_components.code, {\n        children: \"ollama run\"\n      }), \" suivi du nom du modèle.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Par exemple, pour démarrer \", _jsx(_components.a, {\n        href: \"https://mistral.ai\",\n        children: \"Mistral\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama run mistral\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Une fois le modèle démarré, vous pouvez directement interagir avec lui depuis votre terminal.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour supprimer le modèle :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama rm mistral\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il existe même une commande pour démarrer Ollama en mode serveur avec \", _jsx(_components.a, {\n        href: \"https://hub.docker.com/r/ollama/ollama\",\n        children: \"Docker\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"docker run -d --name ollama --restart=always -v ~/.ollama:/root/.ollama -p 11434:11434 ollama/ollama\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vous pouvez interagir avec Ollama via le port \", _jsx(_components.code, {\n        children: \"11434\"\n      }), \" avec des requêtes HTTP :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"curl -X POST http://localhost:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral\\\",\\n  \\\"prompt\\\":\\\"Here is a story about llamas eating grass\\\"\\n}'\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation des modèles HuggingFace au format .gguf\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Et si vous voulez utiliser \", _jsx(_components.a, {\n        href: \"https://huggingface.co/models?search=gguf\",\n        children: \"un modèle au format .gguf\"\n      }), \", vous pouvez le faire :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation avec un client web\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il est aussi possible d'utiliser un client web comme \", _jsx(_components.a, {\n        href: \"https://github.com/open-webui/open-webui\",\n        children: \"Open WebUI\"\n      }), \", \", _jsx(_components.a, {\n        href: \"https://github.com/mckaywrigley/chatbot-ui\",\n        children: \"Chatbot UI\"\n      }), \" ou \", _jsx(_components.a, {\n        href: \"https://github.com/lobehub/lobe-chat\",\n        children: \"Lobe Chat\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"https://i.imgur.com/dprcPpW.png\",\n        alt: \"Open WebUI\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cela donne un rendu très équivalent à ChatGPT.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Nos données restent privées et l'on peut discuter avec un modèle d'IA sans être censuré.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Désinstallation\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour désintaller Ollama :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"sudo systemctl disable --now ollama\\nsudo rm -rf /var/lib/ollama\\nsudo pacman -Rsn ollama\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Serverless GPU\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      className: \"callout note\",\n      children: [\"\\n                    \\n                        \", _jsxs(_components.div, {\n        className: \"callout-title\",\n        children: [\"\\n                            \", _jsx(_components.div, {\n          className: \"callout-icon\",\n          children: _jsxs(_components.svg, {\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"24\",\n            height: \"24\",\n            viewBox: \"0 0 24 24\",\n            fill: \"none\",\n            stroke: \"currentColor\",\n            strokeWidth: \"2\",\n            strokeLinecap: \"round\",\n            strokeLinejoin: \"round\",\n            children: [_jsx(_components.line, {\n              x1: \"18\",\n              y1: \"2\",\n              x2: \"22\",\n              y2: \"6\"\n            }), _jsx(_components.path, {\n              d: \"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"\n            })]\n          })\n        }), \"\\n                            \", _jsx(_components.div, {\n          className: \"callout-title-inner\",\n          children: \"note\"\n        }), \"\\n                        \"]\n      }), \"\\n                    \\n                    \", _jsxs(_components.div, {\n        className: \"callout-content\",\n        children: [\"\\n                        \", _jsx(_components.p, {\n          children: \"En cours de création\"\n        }), \"\\n                    \"]\n      }), \"\\n                \"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Comment utiliser \", _jsx(_components.code, {\n        children: \"ollama run \u003cmodel\u003e\"\n      }), \" (ou open-webui) avec un serveur GPU distant uniquement lorsque une requête est envoyé ?\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"OLLAMA_HOST=https://my.proxy.com ollama run deepseek-r1\\n# Use a proxy\\n# Use runpod\\n# \\\"Ollama is running\\\" on http://\u003crunpod_ip\u003e:11434/\\n# https://github.com/marknefedov/ollama-openrouter-proxy\\n\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"Références :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.geeek.org/mistral-ollama/\",\n          children: \"https://www.geeek.org/mistral-ollama/\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/\",\n          children: \"https://www.geeek.org/tutoriel-installation-llama-2-et-code-llama/\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama\",\n          children: \"https://danielmiessler.com/p/how-to-use-hugging-face-models-with-ollama\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{"title":"Ollama","permalink":"ollama","description":"Ollama est un outil pour utiliser des modèles d'IA (Llama 2, Mistral, etc...) localement.","datePublished":"2024-02-19T10:00:00","dateUpdated":"2024-10-16T10:00:00","publish":true,"rss":true},"scope":{}},"isIndex":false,"file":"Ollama.md","fileName":"Ollama","comments":true,"title":"Ollama","permalink":"ollama","description":"Ollama est un outil pour utiliser des modèles d'IA (Llama 2, Mistral, etc...) localement.","datePublished":"2024-02-19T10:00:00","dateUpdated":"2024-10-16T10:00:00","publish":true,"rss":true},"__N_SSG":true},"page":"/[[...permalink]]","query":{"permalink":["ollama"]},"buildId":"waumMbaWDFC6597IkvKuH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>