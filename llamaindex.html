<!DOCTYPE html><html lang="fr"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@_johackim" data-next-head=""/><meta name="twitter:creator" content="@_johackim" data-next-head=""/><meta property="og:url" content="https://johackim.com" data-next-head=""/><meta property="og:locale" content="fr_FR" data-next-head=""/><meta property="og:site_name" content="johackim" data-next-head=""/><title data-next-head="">LlamaIndex | Johackim - Hacker ind√©pendant</title><meta name="robots" content="index,follow" data-next-head=""/><meta name="description" content="LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM." data-next-head=""/><meta property="og:title" content="LlamaIndex" data-next-head=""/><meta property="og:description" content="LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM." data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="article:published_time" content="2024-04-29T10:00:00.000Z" data-next-head=""/><meta property="article:modified_time" content="2024-04-29T10:00:00.000Z" data-next-head=""/><meta property="og:image" content="https://johackim.com/covers/llamaindex.jpg" data-next-head=""/><link rel="preload" href="/_next/static/media/ce62453a442c7f35-s.p.a9507876.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/chunks/8ed76ae3e03c2690.css" as="style"/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"article","datePublished":"2024-04-29T10:00:00.000Z","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","mainEntityOfPage":{"@type":"WebPage","@id":"https://johackim.com/llamaindex"},"headline":"LlamaIndex","image":["https://johackim.com/covers/llamaindex.jpg"],"dateModified":"2024-04-29T10:00:00.000Z","author":{"@type":"Person","name":"Johackim"}}</script><link rel="stylesheet" href="/_next/static/chunks/8ed76ae3e03c2690.css" data-n-g=""/><noscript data-n-css=""></noscript><script src="/_next/static/chunks/256d18e9d5eb6156.js" defer=""></script><script src="/_next/static/chunks/217963e9d500d59b.js" defer=""></script><script src="/_next/static/chunks/55735a38717165a8.js" defer=""></script><script src="/_next/static/chunks/920f95d0dbc15f96.js" defer=""></script><script src="/_next/static/chunks/turbopack-d1a23f74d30c06bf.js" defer=""></script><script src="/_next/static/chunks/3cd56da0b1290990.js" defer=""></script><script src="/_next/static/chunks/8386287a6b3f85c6.js" defer=""></script><script src="/_next/static/chunks/turbopack-d0d3306c3c212fe2.js" defer=""></script><script src="/_next/static/5g8vQ4SpKJGVr-j-2_lx-/_ssgManifest.js" defer=""></script><script src="/_next/static/5g8vQ4SpKJGVr-j-2_lx-/_buildManifest.js" defer=""></script><style id="__jsx-1922640940">html{font-family:'Roboto', 'Roboto Fallback'}</style></head><body><div id="__next"><header class="flex shadow-md inset-x-0 h-16 items-center z-30 text-gray-700 bg-white fixed top-0"><div class="container m-auto flex items-center justify-between flex-wrap px-2 sm:px-4 lg:max-w-screen-lg"><a href="/"><div class="flex items-center"><img alt="logo" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="rounded-full w-12" style="color:transparent" src="/profile.jpg"/><div class="ml-2"><div class="font-bold leading-none">Johackim</div><div class="text-sm leading-none">Hacker ind√©pendant</div></div></div></a><nav class="grid grid-flow-col gap-4 items-center"><a class="hover:underline hidden md:block" href="/">Accueil</a><a class="hover:underline md:block" href="/articles">Articles</a><a href="/newsletter"><button type="button" class="bg-cyan-700 text-white hover:text-white hover:bg-cyan-800 px-2.5 py-1.5 rounded-md cursor-pointer">S&#x27;abonner</button></a></nav></div></header><main class="lg:max-w-screen-lg m-auto px-4"><div class="bg-cyan-600 h-1 z-30 fixed inset-0" style="width:0%"></div><div class="md:border md:border-gray-200 mt-20"><h1 class="h-64 flex flex-col justify-center relative border-b border-gray-200"><span class="absolute inset-0 bg-gray-100 opacity-80 z-10"></span><span class="transform text-center font-bold px-4 text-4xl text-gray-600 break-words z-20">LlamaIndex</span></h1><div class="text-xs my-4 md:px-4 text-gray-700"><span>Mis √† jour le¬†</span><span>lundi 29 avril 2024</span><span>¬†par¬†<a class="underline text-cyan-700" href="/a-propos">johackim</a></span></div><article class="prose break-words prose-a:font-normal prose-a:text-cyan-700 prose-a:break-words marker:text-gray-700 prose-code:font-normal prose-code:break-words prose-inline-code:px-1.5 prose-inline-code:py-0.5 prose-code:whitespace-pre-wrap prose-code:text-xs prose-code:bg-gray-200 prose-code:rounded-md prose-pre:bg-gray-200 prose-pre:text-gray-700 prose-pre:overflow-x-auto max-w-none px-0 py-4 md:p-4 prose-code:before:hidden prose-code:after:hidden prose-mark:bg-gray-300 prose-td:border-gray-300 prose-td:border prose-td:px-4 prose-th:border prose-th:border-gray-300 prose-th:px-4 prose-th:py-2 [&amp;_blockquote_.callout-title]:flex [&amp;_blockquote_.callout-title]:gap-2"><p><a class="" target="_blank" href="https://llamaindex.ai/">LlamaIndex</a> est un framework open-source pour indexer des donn√©es et les rendre accessibles par un <a class="" href="/llm">LLM</a>.</p>

<p>Cela vous permet de poser des questions √† vos fichiers textes (<code>.txt</code>, <code>.md</code>, <code>.pdf</code>, <code>.epub</code>, etc...).</p>
<p>Et avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n&#x27;importe quel type de donn√©es.</p>
<p>Il existe une <a class="" target="_blank" href="https://github.com/run-llama/llama_index">version Python</a> et une <a class="" target="_blank" href="https://github.com/run-llama/LlamaIndexTS">version JavaScript</a>.</p>
<blockquote class="callout note">
                    
                        <div class="callout-title">
                            <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="2" x2="22" y2="6"></line><path d="M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z"></path></svg></div>
                            <div class="callout-title-inner">note</div>
                        </div>
                    
                    <div class="callout-content">
                        <p>Bien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d&#x27;OpenAI. Donc n&#x27;oubliez pas que chaque requ√™te √† l&#x27;API d&#x27;OpenAI est factur√©e üòâ</p>
                    </div>
                </blockquote>
<h2>Installation</h2>
<p>Commencez par cr√©er un compte sur <a class="" target="_blank" href="https://platform.openai.com/">OpenAI</a> pour pouvoir utiliser l&#x27;API GPT-3.</p>
<p>Puis exportez <a class="" target="_blank" href="https://platform.openai.com/api-keys">votre cl√© d&#x27;API</a> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">export OPENAI_API_KEY=&quot;votre-cl√©-d&#x27;API&quot;
</code></pre></div>
<p>Pour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">npm i llamaindex
</code></pre></div>
<p>Pour la version Python, installez le package <code>llama_index</code> avec <code>pip</code> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">pip install llama_index
</code></pre></div>
<h2>Utilisation en JavaScript</h2>
<p>Cr√©ez un fichier <code>index.mjs</code> avec le contenu suivant :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-js">// index.mjs
import fs from &#x27;fs&#x27;;
import { Document, VectorStoreIndex } from &#x27;llamaindex&#x27;;

const essay = fs.readFileSync(&#x27;node_modules/llamaindex/examples/abramov.txt&#x27;, &#x27;utf-8&#x27;);
const document = new Document({ text: essay });

const index = await VectorStoreIndex.fromDocuments([document]);

const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({
  query: &#x27;What did the author do in college?&#x27;,
});

console.log(response.toString());
</code></pre></div>
<p>Cela permet de lire un fichier texte au format <code>.txt</code> et de poser une question sur son contenu :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">node index.mjs
</code></pre></div>
<p>Pour faire la m√™me chose avec un fichier au format <code>.pdf</code>, il faut utiliser le Data Loader <code>PDFReader</code> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-js">// index.mjs
import { PDFReader, VectorStoreIndex } from &#x27;llamaindex&#x27;;

const reader = new PDFReader();
const documents = await reader.loadData(&#x27;book.pdf&#x27;);

const index = await VectorStoreIndex.fromDocuments(documents);

const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({
  query: &#x27;Give me a summary of this book&#x27;,
});

console.log(response.toString());
</code></pre></div>
<h2>Utilisation en Python</h2>
<p>Pour Python, cr√©ez un dossier <code>data</code> avec des fichiers texte au format <code>.txt</code> ou <code>.pdf</code> et le fichier <code>index.py</code> suivant :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python"># index.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of this book&quot;)

print(response)
</code></pre></div>
<p>Voici un exemple pour charger un autre type de fichier (ex: <code>.epub</code>) :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex
from llama_index.readers.file import EpubReader

documents = EpubReader().load_data(&quot;./book.epub&quot;)
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of the book&quot;)

print(response)
</code></pre></div>
<p>Pour persister l&#x27;index, et √©viter d&#x27;indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (<code>index.py</code> et <code>query.py</code>) et utiliser la m√©thode <code>save</code> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python"># index.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents)
index.storage_context.persist()

# query.py
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir=&quot;./storage&quot;)
index = load_index_from_storage(storage_context)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of the book&quot;)

print(response)
</code></pre></div>
<p>Pour utiliser un autre <a class="" href="/llm">LLM</a> que GPT-3, vous pouvez utiliser <a class="" href="/ollama">Ollama</a> avec l&#x27;argument <code>llm</code> de la fonction <code>as_query_engine</code> :</p>

<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex
from llama_index.readers.file import EpubReader
from llama_index.llms.ollama import Ollama

documents = EpubReader().load_data(&quot;./book.epub&quot;)
index = VectorStoreIndex.from_documents(documents)

llama = Ollama(
    model=&quot;llama2&quot;,
    request_timeout=40.0,
)

query_engine = index.as_query_engine(llm=llama)
res = query_engine.query(&quot;Give me a summary of the book&quot;)

print(res)
</code></pre></div>
<p>Et pour utiliser un autre <em>embedding model</em> que celui d&#x27;OpenAI :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name=&quot;mixedbread-ai/mxbai-embed-large-v1&quot;)

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
index.storage_context.persist()
</code></pre></div>
<p>L&#x27;avantage avec Python, c&#x27;est qu&#x27;il existe beaucoup plus de <a class="" target="_blank" href="https://llamahub.ai/?tab=readers">Data Loaders</a> qu&#x27;avec JavaScript pour charger diff√©rents types de donn√©es :</p>
<ul>
<li><code>EPUBReader</code> (pour les fichiers <code>.epub</code>)</li>
<li><code>VideoAudioReader</code> (pour les fichiers <code>.mp4</code> et <code>.mp3</code>)</li>
<li><code>ImageReader</code> (pour les fichiers <code>.png</code> et <code>.jpg</code>)</li>
<li><code>RSSReader</code> (pour les flux RSS)</li>
<li>etc...</li>
</ul>
<p>Pour aller plus loin, voici <a class="" target="_blank" href="https://docs.llamaindex.ai/en/stable/">la documentation de LlamaIndex</a>.</p>
<hr/>
<p>R√©f√©rences :</p>
<ul>
<li><a class="" target="_blank" href="https://github.com/run-llama/llama_index">https://github.com/run-llama/llama_index</a></li>
<li><a class="" target="_blank" href="https://huggingface.co/learn/cookbook/rag_llamaindex_librarian">https://huggingface.co/learn/cookbook/rag_llamaindex_librarian</a></li>
<li><a class="" target="_blank" href="https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb">https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb</a></li>
</ul></article><div class="md:px-4"><hr class="border-gray-200 mb-4 mt-8"/><div id="commento"></div></div></div></main><footer class="container m-auto px-4 py-8 flex items-center justify-between flex-wrap lg:max-w-screen-lg text-gray-700"><div><span class="inline-block transform rotate-180">¬©</span><span class="ml-2">2017-2026</span></div><nav class="grid grid-flow-col gap-2 items-center"><a href="https://x.com/_johackim" aria-label="X" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a><a href="https://t.me/johackim" aria-label="Telegram" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z"></path></svg></a><a href="https://mastodon.ethibox.fr/@johackim" aria-label="Mastodon" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"></path></svg></a><a href="https://github.com/johackim" aria-label="Github" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a href="https://linkedin.com/in/johackim" aria-label="LinkedIn" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a href="mailto:contact+blog@johackim.com" aria-label="Mail" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="/rss.xml" aria-label="RSS" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg></a></nav></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    div: \"div\",\n    em: \"em\",\n    h2: \"h2\",\n    hr: \"hr\",\n    li: \"li\",\n    line: \"line\",\n    p: \"p\",\n    path: \"path\",\n    pre: \"pre\",\n    svg: \"svg\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://llamaindex.ai/\",\n        children: \"LlamaIndex\"\n      }), \" est un framework open-source pour indexer des donn√©es et les rendre accessibles par un \", _jsx(_components.a, {\n        href: \"/llm\",\n        title: \"LLM\",\n        children: \"LLM\"\n      }), \".\"]\n    }), \"\\n\\n\", _jsxs(_components.p, {\n      children: [\"Cela vous permet de poser des questions √† vos fichiers textes (\", _jsx(_components.code, {\n        children: \".txt\"\n      }), \", \", _jsx(_components.code, {\n        children: \".md\"\n      }), \", \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \", \", _jsx(_components.code, {\n        children: \".epub\"\n      }), \", etc...).\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Et avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n'importe quel type de donn√©es.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Il existe une \", _jsx(_components.a, {\n        href: \"https://github.com/run-llama/llama_index\",\n        children: \"version Python\"\n      }), \" et une \", _jsx(_components.a, {\n        href: \"https://github.com/run-llama/LlamaIndexTS\",\n        children: \"version JavaScript\"\n      }), \".\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      className: \"callout note\",\n      children: [\"\\n                    \\n                        \", _jsxs(_components.div, {\n        className: \"callout-title\",\n        children: [\"\\n                            \", _jsx(_components.div, {\n          className: \"callout-icon\",\n          children: _jsxs(_components.svg, {\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"24\",\n            height: \"24\",\n            viewBox: \"0 0 24 24\",\n            fill: \"none\",\n            stroke: \"currentColor\",\n            strokeWidth: \"2\",\n            strokeLinecap: \"round\",\n            strokeLinejoin: \"round\",\n            children: [_jsx(_components.line, {\n              x1: \"18\",\n              y1: \"2\",\n              x2: \"22\",\n              y2: \"6\"\n            }), _jsx(_components.path, {\n              d: \"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"\n            })]\n          })\n        }), \"\\n                            \", _jsx(_components.div, {\n          className: \"callout-title-inner\",\n          children: \"note\"\n        }), \"\\n                        \"]\n      }), \"\\n                    \\n                    \", _jsxs(_components.div, {\n        className: \"callout-content\",\n        children: [\"\\n                        \", _jsx(_components.p, {\n          children: \"Bien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d'OpenAI. Donc n'oubliez pas que chaque requ√™te √† l'API d'OpenAI est factur√©e üòâ\"\n        }), \"\\n                    \"]\n      }), \"\\n                \"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Installation\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Commencez par cr√©er un compte sur \", _jsx(_components.a, {\n        href: \"https://platform.openai.com/\",\n        children: \"OpenAI\"\n      }), \" pour pouvoir utiliser l'API GPT-3.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Puis exportez \", _jsx(_components.a, {\n        href: \"https://platform.openai.com/api-keys\",\n        children: \"votre cl√© d'API\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"export OPENAI_API_KEY=\\\"votre-cl√©-d'API\\\"\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Pour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"npm i llamaindex\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour la version Python, installez le package \", _jsx(_components.code, {\n        children: \"llama_index\"\n      }), \" avec \", _jsx(_components.code, {\n        children: \"pip\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"pip install llama_index\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation en JavaScript\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Cr√©ez un fichier \", _jsx(_components.code, {\n        children: \"index.mjs\"\n      }), \" avec le contenu suivant :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-js\",\n        children: \"// index.mjs\\nimport fs from 'fs';\\nimport { Document, VectorStoreIndex } from 'llamaindex';\\n\\nconst essay = fs.readFileSync('node_modules/llamaindex/examples/abramov.txt', 'utf-8');\\nconst document = new Document({ text: essay });\\n\\nconst index = await VectorStoreIndex.fromDocuments([document]);\\n\\nconst queryEngine = index.asQueryEngine();\\nconst response = await queryEngine.query({\\n  query: 'What did the author do in college?',\\n});\\n\\nconsole.log(response.toString());\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Cela permet de lire un fichier texte au format \", _jsx(_components.code, {\n        children: \".txt\"\n      }), \" et de poser une question sur son contenu :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"node index.mjs\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour faire la m√™me chose avec un fichier au format \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \", il faut utiliser le Data Loader \", _jsx(_components.code, {\n        children: \"PDFReader\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-js\",\n        children: \"// index.mjs\\nimport { PDFReader, VectorStoreIndex } from 'llamaindex';\\n\\nconst reader = new PDFReader();\\nconst documents = await reader.loadData('book.pdf');\\n\\nconst index = await VectorStoreIndex.fromDocuments(documents);\\n\\nconst queryEngine = index.asQueryEngine();\\nconst response = await queryEngine.query({\\n  query: 'Give me a summary of this book',\\n});\\n\\nconsole.log(response.toString());\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Utilisation en Python\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour Python, cr√©ez un dossier \", _jsx(_components.code, {\n        children: \"data\"\n      }), \" avec des fichiers texte au format \", _jsx(_components.code, {\n        children: \".txt\"\n      }), \" ou \", _jsx(_components.code, {\n        children: \".pdf\"\n      }), \" et le fichier \", _jsx(_components.code, {\n        children: \"index.py\"\n      }), \" suivant :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"# index.py\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of this book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Voici un exemple pour charger un autre type de fichier (ex: \", _jsx(_components.code, {\n        children: \".epub\"\n      }), \") :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex\\nfrom llama_index.readers.file import EpubReader\\n\\ndocuments = EpubReader().load_data(\\\"./book.epub\\\")\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour persister l'index, et √©viter d'indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (\", _jsx(_components.code, {\n        children: \"index.py\"\n      }), \" et \", _jsx(_components.code, {\n        children: \"query.py\"\n      }), \") et utiliser la m√©thode \", _jsx(_components.code, {\n        children: \"save\"\n      }), \" :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"# index.py\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\nindex.storage_context.persist()\\n\\n# query.py\\nfrom llama_index.core import StorageContext, load_index_from_storage\\n\\nstorage_context = StorageContext.from_defaults(persist_dir=\\\"./storage\\\")\\nindex = load_index_from_storage(storage_context)\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(response)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour utiliser un autre \", _jsx(_components.a, {\n        href: \"/llm\",\n        title: \"LLM\",\n        children: \"LLM\"\n      }), \" que GPT-3, vous pouvez utiliser \", _jsx(_components.a, {\n        href: \"/ollama\",\n        title: \"Ollama\",\n        children: \"Ollama\"\n      }), \" avec l'argument \", _jsx(_components.code, {\n        children: \"llm\"\n      }), \" de la fonction \", _jsx(_components.code, {\n        children: \"as_query_engine\"\n      }), \" :\"]\n    }), \"\\n\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex\\nfrom llama_index.readers.file import EpubReader\\nfrom llama_index.llms.ollama import Ollama\\n\\ndocuments = EpubReader().load_data(\\\"./book.epub\\\")\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nllama = Ollama(\\n    model=\\\"llama2\\\",\\n    request_timeout=40.0,\\n)\\n\\nquery_engine = index.as_query_engine(llm=llama)\\nres = query_engine.query(\\\"Give me a summary of the book\\\")\\n\\nprint(res)\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Et pour utiliser un autre \", _jsx(_components.em, {\n        children: \"embedding model\"\n      }), \" que celui d'OpenAI :\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\\\"mixedbread-ai/mxbai-embed-large-v1\\\")\\n\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\\nindex.storage_context.persist()\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"L'avantage avec Python, c'est qu'il existe beaucoup plus de \", _jsx(_components.a, {\n        href: \"https://llamahub.ai/?tab=readers\",\n        children: \"Data Loaders\"\n      }), \" qu'avec JavaScript pour charger diff√©rents types de donn√©es :\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"EPUBReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".epub\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"VideoAudioReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".mp4\"\n        }), \" et \", _jsx(_components.code, {\n          children: \".mp3\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"ImageReader\"\n        }), \" (pour les fichiers \", _jsx(_components.code, {\n          children: \".png\"\n        }), \" et \", _jsx(_components.code, {\n          children: \".jpg\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"RSSReader\"\n        }), \" (pour les flux RSS)\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"etc...\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Pour aller plus loin, voici \", _jsx(_components.a, {\n        href: \"https://docs.llamaindex.ai/en/stable/\",\n        children: \"la documentation de LlamaIndex\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"R√©f√©rences :\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/run-llama/llama_index\",\n          children: \"https://github.com/run-llama/llama_index\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://huggingface.co/learn/cookbook/rag_llamaindex_librarian\",\n          children: \"https://huggingface.co/learn/cookbook/rag_llamaindex_librarian\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb\",\n          children: \"https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{"title":"LlamaIndex","permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true,"note":91},"scope":{}},"isIndex":false,"file":"LlamaIndex.md","fileName":"LlamaIndex","title":"LlamaIndex","comments":true,"permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true,"note":"91"},"__N_SSG":true},"page":"/[[...permalink]]","query":{"permalink":["llamaindex"]},"buildId":"5g8vQ4SpKJGVr-j-2_lx-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>