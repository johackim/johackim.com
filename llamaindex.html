<!DOCTYPE html><html lang="fr"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@_johackim" data-next-head=""/><meta name="twitter:creator" content="@_johackim" data-next-head=""/><meta property="og:url" content="https://johackim.com" data-next-head=""/><meta property="og:locale" content="fr_FR" data-next-head=""/><meta property="og:site_name" content="johackim" data-next-head=""/><title data-next-head="">LlamaIndex | Johackim - Hacker ind√©pendant</title><meta name="robots" content="index,follow" data-next-head=""/><meta name="description" content="LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM." data-next-head=""/><meta property="og:title" content="LlamaIndex" data-next-head=""/><meta property="og:description" content="LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM." data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="article:published_time" content="2024-04-29T10:00:00.000Z" data-next-head=""/><meta property="article:modified_time" content="2024-04-29T10:00:00.000Z" data-next-head=""/><meta property="og:image" content="https://johackim.com/covers/llamaindex.jpg" data-next-head=""/><link rel="preload" href="/_next/static/media/ce62453a442c7f35-s.p.a9507876.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/chunks/8ed76ae3e03c2690.css" as="style"/><link rel="stylesheet" href="/_next/static/chunks/8ed76ae3e03c2690.css" data-n-g=""/><noscript data-n-css=""></noscript><script src="/_next/static/chunks/97f1dd4e262d3e45.js" defer=""></script><script src="/_next/static/chunks/8f7fdb01d15a8219.js" defer=""></script><script src="/_next/static/chunks/55735a38717165a8.js" defer=""></script><script src="/_next/static/chunks/a249cebd710f48a2.js" defer=""></script><script src="/_next/static/chunks/turbopack-97de175104240c95.js" defer=""></script><script src="/_next/static/chunks/615902924f16b9ba.js" defer=""></script><script src="/_next/static/chunks/35397db3c735b8cf.js" defer=""></script><script src="/_next/static/chunks/c9f672f0cfe9662d.js" defer=""></script><script src="/_next/static/chunks/turbopack-7d6231749b9d5e15.js" defer=""></script><script src="/_next/static/37kNBSZEarMk2zx1j6Bqx/_ssgManifest.js" defer=""></script><script src="/_next/static/37kNBSZEarMk2zx1j6Bqx/_buildManifest.js" defer=""></script><style id="__jsx-1922640940">html{font-family:'Roboto', 'Roboto Fallback'}</style></head><body><div id="__next"><header class="flex shadow-md inset-x-0 h-16 items-center z-30 text-gray-700 bg-white fixed top-0"><div class="container m-auto flex items-center justify-between flex-wrap px-2 sm:px-4 lg:max-w-screen-lg"><a href="/"><div class="flex items-center"><img alt="logo" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="rounded-full w-12" style="color:transparent" src="/profile.jpg"/><div class="ml-2"><div class="font-bold leading-none">Johackim</div><div class="text-sm leading-none">Hacker ind√©pendant</div></div></div></a><nav class="grid grid-flow-col gap-4 items-center"><a class="hover:underline hidden md:block" href="/">Accueil</a><a class="hover:underline md:block" href="/articles">Articles</a><a href="/newsletter"><button type="button" class="bg-cyan-700 text-white hover:text-white hover:bg-cyan-800 px-2.5 py-1.5 rounded-md cursor-pointer">S&#x27;abonner</button></a></nav></div></header><main class="lg:max-w-screen-lg m-auto px-4"><div class="bg-cyan-600 h-1 z-30 fixed inset-0" style="width:0%"></div><script type="application/ld+json" id="article-jsonld-article">{"@context":"https://schema.org","@type":"article","headline":"LlamaIndex","url":"https://johackim.com/llamaindex","author":{"@type":"Person","name":"Johackim"},"datePublished":"2024-04-29T10:00:00.000Z","dateModified":"2024-04-29T10:00:00.000Z","image":"https://johackim.com/covers/llamaindex.jpg","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM."}</script><div class="md:border md:border-gray-200 mt-20"><h1 class="h-64 flex flex-col justify-center relative border-b border-gray-200"><span class="absolute inset-0 bg-gray-100 opacity-80 z-10"></span><span class="transform text-center font-bold px-4 text-4xl text-gray-600 break-words z-20">LlamaIndex</span></h1><div class="text-xs my-4 md:px-4 text-gray-700"><span>Mis √† jour le¬†</span><span>lundi 29 avril 2024</span><span>¬†par¬†<a class="underline text-cyan-700" href="/a-propos">johackim</a></span></div><article class="prose break-words prose-a:font-normal prose-a:text-cyan-700 prose-a:break-words marker:text-gray-700 prose-code:font-normal prose-code:break-words prose-inline-code:px-1.5 prose-inline-code:py-0.5 prose-code:whitespace-pre-wrap prose-code:text-xs prose-code:bg-gray-200 prose-code:rounded-md prose-pre:bg-gray-200 prose-pre:text-gray-700 prose-pre:overflow-x-auto max-w-none px-0 py-4 md:p-4 prose-code:before:hidden prose-code:after:hidden prose-mark:bg-gray-300 prose-td:border-gray-300 prose-td:border prose-td:px-4 prose-th:border prose-th:border-gray-300 prose-th:px-4 prose-th:py-2 [&amp;_blockquote_.callout-title]:flex [&amp;_blockquote_.callout-title]:gap-2"><div class="space-y-4 whitespace-normal *:first:mt-0 *:last:mb-0"><p><a class="" target="_blank" href="https://llamaindex.ai/">LlamaIndex</a> est un framework open-source pour indexer des donn√©es et les rendre accessibles par un <a class="" href="/llm">LLM</a>.</p>
<p>Cela vous permet de poser des questions √† vos fichiers textes (<code>.txt</code>, <code>.md</code>, <code>.pdf</code>, <code>.epub</code>, etc...).</p><p>Et avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n&#x27;importe quel type de donn√©es.</p><p>Il existe une <a class="" target="_blank" href="https://github.com/run-llama/llama_index">version Python</a> et une <a class="" target="_blank" href="https://github.com/run-llama/LlamaIndexTS">version JavaScript</a>.</p><blockquote class="callout note">
                    
                        <div class="callout-title">
                            <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="2" x2="22" y2="6"></line><path d="M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z"></path></svg></div>
                            <div class="callout-title-inner">note</div>
                        </div>
                    
                    <div class="callout-content">
                        <p>Bien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d&#x27;OpenAI. Donc n&#x27;oubliez pas que chaque requ√™te √† l&#x27;API d&#x27;OpenAI est factur√©e üòâ</p>
                    </div>
                </blockquote><h2>Installation</h2><p>Commencez par cr√©er un compte sur <a class="" target="_blank" href="https://platform.openai.com/">OpenAI</a> pour pouvoir utiliser l&#x27;API GPT-3.</p><p>Puis exportez <a class="" target="_blank" href="https://platform.openai.com/api-keys">votre cl√© d&#x27;API</a> :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">export OPENAI_API_KEY=&quot;votre-cl√©-d&#x27;API&quot;
</code></pre></div><p>Pour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">npm i llamaindex
</code></pre></div><p>Pour la version Python, installez le package <code>llama_index</code> avec <code>pip</code> :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">pip install llama_index
</code></pre></div><h2>Utilisation en JavaScript</h2><p>Cr√©ez un fichier <code>index.mjs</code> avec le contenu suivant :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-js">// index.mjs
import fs from &#x27;fs&#x27;;
import { Document, VectorStoreIndex } from &#x27;llamaindex&#x27;;

const essay = fs.readFileSync(&#x27;node_modules/llamaindex/examples/abramov.txt&#x27;, &#x27;utf-8&#x27;);
const document = new Document({ text: essay });

const index = await VectorStoreIndex.fromDocuments([document]);

const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({
  query: &#x27;What did the author do in college?&#x27;,
});

console.log(response.toString());
</code></pre></div><p>Cela permet de lire un fichier texte au format <code>.txt</code> et de poser une question sur son contenu :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-bash">node index.mjs
</code></pre></div><p>Pour faire la m√™me chose avec un fichier au format <code>.pdf</code>, il faut utiliser le Data Loader <code>PDFReader</code> :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-js">// index.mjs
import { PDFReader, VectorStoreIndex } from &#x27;llamaindex&#x27;;

const reader = new PDFReader();
const documents = await reader.loadData(&#x27;book.pdf&#x27;);

const index = await VectorStoreIndex.fromDocuments(documents);

const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({
  query: &#x27;Give me a summary of this book&#x27;,
});

console.log(response.toString());
</code></pre></div><h2>Utilisation en Python</h2><p>Pour Python, cr√©ez un dossier <code>data</code> avec des fichiers texte au format <code>.txt</code> ou <code>.pdf</code> et le fichier <code>index.py</code> suivant :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python"># index.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of this book&quot;)

print(response)
</code></pre></div><p>Voici un exemple pour charger un autre type de fichier (ex: <code>.epub</code>) :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex
from llama_index.readers.file import EpubReader

documents = EpubReader().load_data(&quot;./book.epub&quot;)
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of the book&quot;)

print(response)
</code></pre></div><p>Pour persister l&#x27;index, et √©viter d&#x27;indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (<code>index.py</code> et <code>query.py</code>) et utiliser la m√©thode <code>save</code> :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python"># index.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents)
index.storage_context.persist()

# query.py
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir=&quot;./storage&quot;)
index = load_index_from_storage(storage_context)

query_engine = index.as_query_engine()
response = query_engine.query(&quot;Give me a summary of the book&quot;)

print(response)
</code></pre></div><p>Pour utiliser un autre <a class="" href="/llm">LLM</a> que GPT-3, vous pouvez utiliser <a class="" href="/ollama">Ollama</a> avec l&#x27;argument <code>llm</code> de la fonction <code>as_query_engine</code> :</p>
<div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex
from llama_index.readers.file import EpubReader
from llama_index.llms.ollama import Ollama

documents = EpubReader().load_data(&quot;./book.epub&quot;)
index = VectorStoreIndex.from_documents(documents)

llama = Ollama(
    model=&quot;llama2&quot;,
    request_timeout=40.0,
)

query_engine = index.as_query_engine(llm=llama)
res = query_engine.query(&quot;Give me a summary of the book&quot;)

print(res)
</code></pre></div><p>Et pour utiliser un autre <em>embedding model</em> que celui d&#x27;OpenAI :</p><div class="relative group"><button type="button" class="invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none">Copier</button><pre><code class="language-python">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name=&quot;mixedbread-ai/mxbai-embed-large-v1&quot;)

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
index.storage_context.persist()
</code></pre></div><p>L&#x27;avantage avec Python, c&#x27;est qu&#x27;il existe beaucoup plus de <a class="" target="_blank" href="https://llamahub.ai/?tab=readers">Data Loaders</a> qu&#x27;avec JavaScript pour charger diff√©rents types de donn√©es :</p><ul>
<li><code>EPUBReader</code> (pour les fichiers <code>.epub</code>)</li>
<li><code>VideoAudioReader</code> (pour les fichiers <code>.mp4</code> et <code>.mp3</code>)</li>
<li><code>ImageReader</code> (pour les fichiers <code>.png</code> et <code>.jpg</code>)</li>
<li><code>RSSReader</code> (pour les flux RSS)</li>
<li>etc...</li>
</ul><p>Pour aller plus loin, voici <a class="" target="_blank" href="https://docs.llamaindex.ai/en/stable/">la documentation de LlamaIndex</a>.</p><hr/><p>R√©f√©rences :</p><ul>
<li>https://github.com/run-llama/llama_index</li>
<li>https://huggingface.co/learn/cookbook/rag_llamaindex_librarian</li>
<li>https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb</li>
</ul></div></article><div class="md:px-4"><hr class="border-gray-200 mb-4 mt-8"/><div id="commento"></div></div></div></main><footer class="container m-auto px-4 py-8 flex items-center justify-between flex-wrap lg:max-w-screen-lg text-gray-700"><div><span class="inline-block transform rotate-180">¬©</span><span class="ml-2">2017-2026</span></div><nav class="grid grid-flow-col gap-2 items-center"><a href="https://x.com/_johackim" aria-label="X" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a><a href="https://t.me/johackim" aria-label="Telegram" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z"></path></svg></a><a href="https://mastodon.ethibox.fr/@johackim" aria-label="Mastodon" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"></path></svg></a><a href="https://github.com/johackim" aria-label="Github" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a href="https://linkedin.com/in/johackim" aria-label="LinkedIn" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a href="mailto:contact+blog@johackim.com" aria-label="Mail" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="/rss.xml" aria-label="RSS" target="_blank" rel="noreferrer"><svg class="h-5 w-5 fill-current hover:text-black" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg></a></nav></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"source":"\u003cdiv class=\"space-y-4 whitespace-normal *:first:mt-0 *:last:mb-0\"\u003e\u003cp\u003e\u003ca class=\"\" target=\"_blank\" href=\"https://llamaindex.ai/\"\u003eLlamaIndex\u003c/a\u003e est un framework open-source pour indexer des donn√©es et les rendre accessibles par un \u003ca class=\"\" href=\"/llm\"\u003eLLM\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCela vous permet de poser des questions √† vos fichiers textes (\u003ccode\u003e.txt\u003c/code\u003e, \u003ccode\u003e.md\u003c/code\u003e, \u003ccode\u003e.pdf\u003c/code\u003e, \u003ccode\u003e.epub\u003c/code\u003e, etc...).\u003c/p\u003e\u003cp\u003eEt avec les transcripts audio et vid√©o, les flux RSS, etc... vous pouvez indexer et interroger n\u0026#x27;importe quel type de donn√©es.\u003c/p\u003e\u003cp\u003eIl existe une \u003ca class=\"\" target=\"_blank\" href=\"https://github.com/run-llama/llama_index\"\u003eversion Python\u003c/a\u003e et une \u003ca class=\"\" target=\"_blank\" href=\"https://github.com/run-llama/LlamaIndexTS\"\u003eversion JavaScript\u003c/a\u003e.\u003c/p\u003e\u003cblockquote class=\"callout note\"\u003e\n                    \n                        \u003cdiv class=\"callout-title\"\u003e\n                            \u003cdiv class=\"callout-icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\u003cline x1=\"18\" y1=\"2\" x2=\"22\" y2=\"6\"\u003e\u003c/line\u003e\u003cpath d=\"M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/div\u003e\n                            \u003cdiv class=\"callout-title-inner\"\u003enote\u003c/div\u003e\n                        \u003c/div\u003e\n                    \n                    \u003cdiv class=\"callout-content\"\u003e\n                        \u003cp\u003eBien que LLamaIndex peut utiliser des LLMs open-source (ex: Llama3), il fonctionne par d√©faut avec GPT-3 d\u0026#x27;OpenAI. Donc n\u0026#x27;oubliez pas que chaque requ√™te √† l\u0026#x27;API d\u0026#x27;OpenAI est factur√©e üòâ\u003c/p\u003e\n                    \u003c/div\u003e\n                \u003c/blockquote\u003e\u003ch2\u003eInstallation\u003c/h2\u003e\u003cp\u003eCommencez par cr√©er un compte sur \u003ca class=\"\" target=\"_blank\" href=\"https://platform.openai.com/\"\u003eOpenAI\u003c/a\u003e pour pouvoir utiliser l\u0026#x27;API GPT-3.\u003c/p\u003e\u003cp\u003ePuis exportez \u003ca class=\"\" target=\"_blank\" href=\"https://platform.openai.com/api-keys\"\u003evotre cl√© d\u0026#x27;API\u003c/a\u003e :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-bash\"\u003eexport OPENAI_API_KEY=\u0026quot;votre-cl√©-d\u0026#x27;API\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePour installer la version de LlamaIndex en JavaScript, ex√©cutez la commande suivante :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-bash\"\u003enpm i llamaindex\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePour la version Python, installez le package \u003ccode\u003ellama_index\u003c/code\u003e avec \u003ccode\u003epip\u003c/code\u003e :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epip install llama_index\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003eUtilisation en JavaScript\u003c/h2\u003e\u003cp\u003eCr√©ez un fichier \u003ccode\u003eindex.mjs\u003c/code\u003e avec le contenu suivant :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-js\"\u003e// index.mjs\nimport fs from \u0026#x27;fs\u0026#x27;;\nimport { Document, VectorStoreIndex } from \u0026#x27;llamaindex\u0026#x27;;\n\nconst essay = fs.readFileSync(\u0026#x27;node_modules/llamaindex/examples/abramov.txt\u0026#x27;, \u0026#x27;utf-8\u0026#x27;);\nconst document = new Document({ text: essay });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: \u0026#x27;What did the author do in college?\u0026#x27;,\n});\n\nconsole.log(response.toString());\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCela permet de lire un fichier texte au format \u003ccode\u003e.txt\u003c/code\u003e et de poser une question sur son contenu :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-bash\"\u003enode index.mjs\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePour faire la m√™me chose avec un fichier au format \u003ccode\u003e.pdf\u003c/code\u003e, il faut utiliser le Data Loader \u003ccode\u003ePDFReader\u003c/code\u003e :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-js\"\u003e// index.mjs\nimport { PDFReader, VectorStoreIndex } from \u0026#x27;llamaindex\u0026#x27;;\n\nconst reader = new PDFReader();\nconst documents = await reader.loadData(\u0026#x27;book.pdf\u0026#x27;);\n\nconst index = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: \u0026#x27;Give me a summary of this book\u0026#x27;,\n});\n\nconsole.log(response.toString());\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003eUtilisation en Python\u003c/h2\u003e\u003cp\u003ePour Python, cr√©ez un dossier \u003ccode\u003edata\u003c/code\u003e avec des fichiers texte au format \u003ccode\u003e.txt\u003c/code\u003e ou \u003ccode\u003e.pdf\u003c/code\u003e et le fichier \u003ccode\u003eindex.py\u003c/code\u003e suivant :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# index.py\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\u0026quot;./data\u0026quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\u0026quot;Give me a summary of this book\u0026quot;)\n\nprint(response)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eVoici un exemple pour charger un autre type de fichier (ex: \u003ccode\u003e.epub\u003c/code\u003e) :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom llama_index.core import VectorStoreIndex\nfrom llama_index.readers.file import EpubReader\n\ndocuments = EpubReader().load_data(\u0026quot;./book.epub\u0026quot;)\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\u0026quot;Give me a summary of the book\u0026quot;)\n\nprint(response)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePour persister l\u0026#x27;index, et √©viter d\u0026#x27;indexer tous les documents √† chaque ex√©cution, vous pouvez s√©parer le fichier en 2 (\u003ccode\u003eindex.py\u003c/code\u003e et \u003ccode\u003equery.py\u003c/code\u003e) et utiliser la m√©thode \u003ccode\u003esave\u003c/code\u003e :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# index.py\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\u0026quot;./data\u0026quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex.storage_context.persist()\n\n# query.py\nfrom llama_index.core import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\u0026quot;./storage\u0026quot;)\nindex = load_index_from_storage(storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\u0026quot;Give me a summary of the book\u0026quot;)\n\nprint(response)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePour utiliser un autre \u003ca class=\"\" href=\"/llm\"\u003eLLM\u003c/a\u003e que GPT-3, vous pouvez utiliser \u003ca class=\"\" href=\"/ollama\"\u003eOllama\u003c/a\u003e avec l\u0026#x27;argument \u003ccode\u003ellm\u003c/code\u003e de la fonction \u003ccode\u003eas_query_engine\u003c/code\u003e :\u003c/p\u003e\n\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom llama_index.core import VectorStoreIndex\nfrom llama_index.readers.file import EpubReader\nfrom llama_index.llms.ollama import Ollama\n\ndocuments = EpubReader().load_data(\u0026quot;./book.epub\u0026quot;)\nindex = VectorStoreIndex.from_documents(documents)\n\nllama = Ollama(\n    model=\u0026quot;llama2\u0026quot;,\n    request_timeout=40.0,\n)\n\nquery_engine = index.as_query_engine(llm=llama)\nres = query_engine.query(\u0026quot;Give me a summary of the book\u0026quot;)\n\nprint(res)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEt pour utiliser un autre \u003cem\u003eembedding model\u003c/em\u003e que celui d\u0026#x27;OpenAI :\u003c/p\u003e\u003cdiv class=\"relative group\"\u003e\u003cbutton type=\"button\" class=\"invisible group-hover:visible absolute top-0 right-0 rounded-md text-xs bg-gray-300 m-3 p-1.5 cursor-pointer leading-none\"\u003eCopier\u003c/button\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nembed_model = HuggingFaceEmbedding(model_name=\u0026quot;mixedbread-ai/mxbai-embed-large-v1\u0026quot;)\n\ndocuments = SimpleDirectoryReader(\u0026quot;./data\u0026quot;).load_data()\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\nindex.storage_context.persist()\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eL\u0026#x27;avantage avec Python, c\u0026#x27;est qu\u0026#x27;il existe beaucoup plus de \u003ca class=\"\" target=\"_blank\" href=\"https://llamahub.ai/?tab=readers\"\u003eData Loaders\u003c/a\u003e qu\u0026#x27;avec JavaScript pour charger diff√©rents types de donn√©es :\u003c/p\u003e\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eEPUBReader\u003c/code\u003e (pour les fichiers \u003ccode\u003e.epub\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eVideoAudioReader\u003c/code\u003e (pour les fichiers \u003ccode\u003e.mp4\u003c/code\u003e et \u003ccode\u003e.mp3\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eImageReader\u003c/code\u003e (pour les fichiers \u003ccode\u003e.png\u003c/code\u003e et \u003ccode\u003e.jpg\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRSSReader\u003c/code\u003e (pour les flux RSS)\u003c/li\u003e\n\u003cli\u003eetc...\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003ePour aller plus loin, voici \u003ca class=\"\" target=\"_blank\" href=\"https://docs.llamaindex.ai/en/stable/\"\u003ela documentation de LlamaIndex\u003c/a\u003e.\u003c/p\u003e\u003chr/\u003e\u003cp\u003eR√©f√©rences :\u003c/p\u003e\u003cul\u003e\n\u003cli\u003ehttps://github.com/run-llama/llama_index\u003c/li\u003e\n\u003cli\u003ehttps://huggingface.co/learn/cookbook/rag_llamaindex_librarian\u003c/li\u003e\n\u003cli\u003ehttps://github.com/anthropics/anthropic-cookbook/blob/main/third_party/LlamaIndex/Basic_RAG_With_LlamaIndex.ipynb\u003c/li\u003e\n\u003c/ul\u003e\u003c/div\u003e","isIndex":false,"file":"LlamaIndex.md","fileName":"LlamaIndex","title":"LlamaIndex","comments":true,"permalink":"llamaindex","description":"LlamaIndex est un framework open-source pour indexer des donn√©es et les rendre accessibles par un LLM.","datePublished":"2024-04-29T10:00:00","dateUpdated":"2024-04-29T10:00:00","publish":true,"rss":true,"note":"91"},"__N_SSG":true},"page":"/[[...permalink]]","query":{"permalink":["llamaindex"]},"buildId":"37kNBSZEarMk2zx1j6Bqx","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>